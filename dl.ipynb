{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from config import get_dl_config\n",
    "import xarray as xr\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset, Dataset\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import StratifiedGroupKFold\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, confusion_matrix\n",
    "import time\n",
    "import os "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### setup cuda and config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = get_dl_config()\n",
    "SAMPLING_FREQ = conf['sampling_frequency']\n",
    "INPUT_FILE = \"/dhc/home/jannis.hajda/tuh-eeg-seizure-detection/data/preprocessed/windows_time_20_normalized_per_channel.nc\" #conf['input_file']\n",
    "OUTPUT_PATH = \"/dhc/home/jannis.hajda/tuh-eeg-seizure-detection/data/results/\"\n",
    "\n",
    "BATCH_SIZE = 128 #128\n",
    "LR = 1e-3 \n",
    "\n",
    "N_EPOCHS = 100\n",
    "N_SPLITS = 5\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = xr.open_dataarray(INPUT_FILE)\n",
    "data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = data['label'].values\n",
    "patient_ids = data['patient_id'].values\n",
    "measurements = data.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WINDOW_LENGTH = data.sizes[\"time\"] // SAMPLING_FREQ\n",
    "WINDOW_LENGTH"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### create datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TUHDataset(Dataset):\n",
    "    def __init__(self, data, labels):\n",
    "        self.data = torch.tensor(data, dtype=torch.float32)\n",
    "        self.labels = torch.tensor(labels, dtype=torch.float32)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx].unsqueeze(0), self.labels[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### cnn setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleNet(nn.Module):\n",
    "    def __init__(self, domain='time'):\n",
    "        super(SimpleNet, self).__init__()\n",
    "        # input: (batch_size, 1, 19, WINDOW_LENGTH * SAMPLING_FREQ)\n",
    "\n",
    "        self.conv = nn.Conv2d(1, 6, kernel_size=(5, 5), stride=1)         \n",
    "        self.pool = nn.AvgPool2d(kernel_size=(2, 2), stride=2) \n",
    "\n",
    "        self.fc_input_size = 6 * 7 * (((WINDOW_LENGTH * SAMPLING_FREQ) - 4) // 2)\n",
    "        self.fc = nn.Linear(self.fc_input_size, 1)  \n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv(x))\n",
    "        x = self.pool(x)\n",
    "\n",
    "        x = torch.flatten(x, 1)  \n",
    "        x = self.fc(x)\n",
    "\n",
    "        x = torch.sigmoid(x)  \n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### training phase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, dataloader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    correct_predictions = 0\n",
    "    total_samples = 0\n",
    "\n",
    "    start_time = time.time()  \n",
    "\n",
    "    for inputs, targets in dataloader:\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets.unsqueeze(1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        correct_predictions += ((outputs.squeeze() > 0.5) == targets).sum().item()\n",
    "        total_samples += targets.size(0)\n",
    "\n",
    "    epoch_time = time.time() - start_time \n",
    "\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    accuracy = correct_predictions / total_samples\n",
    "\n",
    "    return avg_loss, accuracy, epoch_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_model(model, dataloader, criterion, device):\n",
    "    model.eval()\n",
    "\n",
    "    total_loss = 0.0\n",
    "    correct_predictions = 0\n",
    "    total_samples = 0\n",
    "    all_predictions = []\n",
    "    all_targets = []\n",
    "\n",
    "    start_time = time.time()  \n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in dataloader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets.unsqueeze(1))\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            predictions = (outputs.squeeze() > 0.5).float()\n",
    "            all_predictions.extend(predictions.cpu().numpy())\n",
    "            all_targets.extend(targets.cpu().numpy())\n",
    "\n",
    "            correct_predictions += ((predictions == targets).sum().item())\n",
    "            total_samples += targets.size(0)\n",
    "        \n",
    "    epoch_time = time.time() - start_time \n",
    "\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    accuracy = correct_predictions / total_samples\n",
    "    precision = precision_score(all_targets, all_predictions)\n",
    "    recall = recall_score(all_targets, all_predictions)\n",
    "    f1 = f1_score(all_targets, all_predictions)\n",
    "    roc_auc = roc_auc_score(all_targets, all_predictions)\n",
    "    tn, fp, fn, tp = confusion_matrix(all_targets, all_predictions).ravel()\n",
    "\n",
    "    return avg_loss, accuracy, precision, recall, f1, roc_auc, tn, fp, fn, tp, epoch_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_validate_model(measurements, labels, patient_ids, n_splits, batch_size, n_epochs, learning_rate, device):\n",
    "    sgkf = StratifiedGroupKFold(n_splits=n_splits)\n",
    "    splits = list(sgkf.split(X=measurements, y=labels, groups=patient_ids))\n",
    "    split_metrics = []\n",
    "\n",
    "    # Iterate over the splits\n",
    "    for i, (train_idx, val_idx) in enumerate(splits):\n",
    "        model = SimpleNet().to(device)\n",
    "\n",
    "        # Prepare the data\n",
    "        train_data, train_labels = measurements[train_idx], labels[train_idx]\n",
    "        val_data, val_labels = measurements[val_idx], labels[val_idx]\n",
    "\n",
    "        train_dataset = TUHDataset(train_data, train_labels)\n",
    "        train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "        val_dataset = TUHDataset(val_data, val_labels)\n",
    "        val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "        # Initialize optimizer and loss function\n",
    "        criterion = nn.BCELoss()\n",
    "        optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "        \n",
    "        best_epoch_metrics = {\n",
    "            'epoch': 0,\n",
    "            'val_loss': np.inf,\n",
    "            'val_accuracy': 0.0,\n",
    "            'val_precision': 0.0,\n",
    "            'val_recall': 0.0,\n",
    "            'val_f1': 0.0,\n",
    "            'val_roc_auc': 0.0,\n",
    "            'tn': 0,\n",
    "            'fp': 0,\n",
    "            'fn': 0,\n",
    "            'tp': 0,\n",
    "            'total_train_time': 0.0,\n",
    "            'val_time': 0.0, \n",
    "        }\n",
    "\n",
    "        total_train_time = 0.0\n",
    "        \n",
    "        # Training loop\n",
    "        for epoch in range(n_epochs):\n",
    "            train_loss, train_accuracy, train_time = train_epoch(model, train_dataloader, optimizer, criterion, device)\n",
    "            total_train_time += train_time\n",
    "\n",
    "            val_loss, val_accuracy, val_precision, val_recall, val_f1, val_roc_auc, tn, fp, fn, tp, val_time = validate_model(model, val_dataloader, criterion, device)\n",
    "\n",
    "            if val_loss < best_epoch_metrics['val_loss']:\n",
    "                best_epoch_metrics = {\n",
    "                    'epoch': epoch + 1,\n",
    "                    'val_loss': val_loss,\n",
    "                    'val_accuracy': val_accuracy,\n",
    "                    'val_precision': val_precision,\n",
    "                    'val_recall': val_recall,\n",
    "                    'val_f1': val_f1,\n",
    "                    'val_roc_auc': val_roc_auc, \n",
    "                    \"tn\": tn,\n",
    "                    \"fp\": fp,\n",
    "                    \"fn\": fn,\n",
    "                    \"tp\": tp,\n",
    "                    'total_train_time': total_train_time,\n",
    "                    'val_time': val_time,\n",
    "                }\n",
    "\n",
    "                print(f\"New best model found at epoch {epoch + 1}, \"\n",
    "                      f\"Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, \"\n",
    "                      f\"Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}\")\n",
    "\n",
    "            else:\n",
    "                print(f'Epoch {epoch + 1}/{n_epochs}, '\n",
    "                      f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}, '\n",
    "                      f'Val Loss: {val_loss:.4f}, Val Accuracy: {val_accuracy:.4f}')\n",
    "        \n",
    "        split_metrics.append(best_epoch_metrics)\n",
    "\n",
    "        print(f\"Split {i + 1}/{n_splits}, best model at epoch {best_epoch_metrics['epoch']}, \"\n",
    "              f\"Best Val Loss: {best_epoch_metrics['val_loss']:.4f}, \"\n",
    "              f\"Best Val Accuracy: {best_epoch_metrics['val_accuracy']:.4f}, \")\n",
    "            \n",
    "        # Explicitly delete the data and model to free up memory\n",
    "        del train_data, train_labels, val_data, val_labels\n",
    "        del train_dataset, val_dataset, train_dataloader, val_dataloader\n",
    "        del model, optimizer\n",
    "        torch.cuda.empty_cache()  # Clear the GPU cache\n",
    "\n",
    "    avg_metrics = {\n",
    "        'avg_epoch': np.mean([m['epoch'] for m in split_metrics]),\n",
    "        'avg_val_loss': np.mean([m['val_loss'] for m in split_metrics]),\n",
    "        'avg_val_accuracy': np.mean([m['val_accuracy'] for m in split_metrics]),\n",
    "        'avg_val_precision': np.mean([m['val_precision'] for m in split_metrics]),\n",
    "        'avg_val_recall': np.mean([m['val_recall'] for m in split_metrics]),\n",
    "        'avg_val_f1': np.mean([m['val_f1'] for m in split_metrics]),\n",
    "        'avg_val_roc_auc': np.mean([m['val_roc_auc'] for m in split_metrics]),\n",
    "        'sum_tn': np.sum([m['tn'] for m in split_metrics]),\n",
    "        'sum_fp': np.sum([m['fp'] for m in split_metrics]),\n",
    "        'sum_fn': np.sum([m['fn'] for m in split_metrics]),\n",
    "        'sum_tp': np.sum([m['tp'] for m in split_metrics]),\n",
    "        'avg_total_train_time': np.mean([m['total_train_time'] for m in split_metrics]),\n",
    "        'avg_val_time': np.mean([m['val_time'] for m in split_metrics]),\n",
    "    }\n",
    "\n",
    "    return split_metrics, avg_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_metrics, avg_metrics = cross_validate_model(measurements, labels, patient_ids, N_SPLITS, BATCH_SIZE, N_EPOCHS, LR, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ba_cuda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
