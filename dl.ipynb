{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from config import get_dl_config\n",
    "import xarray as xr\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset, Dataset\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import StratifiedGroupKFold\n",
    "import gc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = get_dl_config()\n",
    "\n",
    "INPUT_FILE = conf['input_file']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TUHDataset(Dataset):\n",
    "    def __init__(self, data, labels):\n",
    "        self.data = torch.tensor(data, dtype=torch.float32)\n",
    "        self.labels = torch.tensor(labels, dtype=torch.float32)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # data needs to be unsqueezed to add window dimension\n",
    "        return self.data[idx].unsqueeze(0), self.labels[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = xr.open_dataarray(INPUT_FILE)\n",
    "data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prepare labels\n",
    "unique_labels = np.unique(data['label'].values)\n",
    "labels_map = {label: i for i, label in enumerate(unique_labels)}\n",
    "labels = np.array([labels_map[label] for label in data['label'].values])\n",
    "labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "groups = data['patient_id'].values\n",
    "measurements = data.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create training data using stratifiedgroupkfold\n",
    "n_splits = 5\n",
    "sgkf = StratifiedGroupKFold(n_splits=n_splits)\n",
    "splits = list(sgkf.split(measurements, labels, groups))\n",
    "\n",
    "# select one split -> 80% train, 20% test\n",
    "train_idx, test_idx = splits[np.random.choice(n_splits)]\n",
    "\n",
    "train_data, train_labels = measurements[train_idx], labels[train_idx]\n",
    "test_data, test_labels = measurements[test_idx], labels[test_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "\n",
    "train_dataset = TUHDataset(train_data, train_labels)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "test_dataset = TUHDataset(test_data, test_labels)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset.data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definition and training of CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ParkNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ParkNet, self).__init__()\n",
    "        \n",
    "        # 1D Convolutions and Max Pooling layers\n",
    "        self.conv1 = nn.Conv1d(1, 8, 3, stride=1)\n",
    "        self.conv2 = nn.Conv1d(8, 8, 3, stride=1)\n",
    "        self.maxpool1 = nn.MaxPool1d(2, stride=2)\n",
    "        self.conv3 = nn.Conv1d(8, 16, 3, stride=1)\n",
    "        self.conv4 = nn.Conv1d(16, 32, 3, stride=1)\n",
    "        self.conv5 = nn.Conv1d(32, 32, 3, stride=1)\n",
    "        self.maxpool2 = nn.MaxPool1d(2, stride=2)\n",
    "        \n",
    "        # 2D Convolutions for different groups\n",
    "        self.conv2d_group1 = nn.Conv2d(32, 32, (4, 3), stride=1)\n",
    "        self.conv2d_group2 = nn.Conv2d(32, 32, (4, 3), stride=1)\n",
    "        self.conv2d_group3 = nn.Conv2d(32, 32, (4, 3), stride=1)\n",
    "        self.conv2d_group4 = nn.Conv2d(32, 32, (4, 3), stride=1)\n",
    "        self.conv2d_group5 = nn.Conv2d(32, 32, (3, 3), stride=1)\n",
    "\n",
    "        # Final 2D Convolution and Max Pooling\n",
    "        self.conv2d_final = nn.Conv2d(32, 128, (5, 3), stride=1)\n",
    "        self.maxpool_final = nn.MaxPool2d((1, 2), stride=(1, 2))\n",
    "        \n",
    "        # Fully connected layers\n",
    "        self.fc1 = nn.Linear(128 * ((1306 - 2) // 2), 512)\n",
    "        self.fc2 = nn.Linear(512, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        groups = {\n",
    "            'group1': [0, 3, 7, 11],\n",
    "            'group2': [1, 4, 8, 12],\n",
    "            'group3': [2, 6, 10, 14],\n",
    "            'group4': [5, 9, 13, 15],\n",
    "            'group5': [16, 17, 18]\n",
    "        }\n",
    "\n",
    "        group_outputs = []\n",
    "\n",
    "        for group in groups.values():\n",
    "            group_channels = []\n",
    "            for channel_idx in group:\n",
    "                channel_data = x[:, :, channel_idx, :]\n",
    "                channel_output = self.conv1(channel_data)\n",
    "                channel_output = F.relu(self.conv2(channel_output))\n",
    "                channel_output = self.maxpool1(channel_output)\n",
    "                channel_output = F.relu(self.conv3(channel_output))\n",
    "                channel_output = F.relu(self.conv4(channel_output))\n",
    "                channel_output = F.relu(self.conv5(channel_output))\n",
    "                channel_output = self.maxpool2(channel_output)\n",
    "                group_channels.append(channel_output)\n",
    "            \n",
    "            group_output = torch.stack(group_channels, dim=2)\n",
    "            if group == groups['group1']:\n",
    "                group_output = self.conv2d_group1(group_output)\n",
    "            elif group == groups['group2']:\n",
    "                group_output = self.conv2d_group2(group_output)\n",
    "            elif group == groups['group3']:\n",
    "                group_output = self.conv2d_group3(group_output)\n",
    "            elif group == groups['group4']:\n",
    "                group_output = self.conv2d_group4(group_output)\n",
    "            elif group == groups['group5']:\n",
    "                group_output = self.conv2d_group5(group_output)\n",
    "                \n",
    "            group_output = F.relu(group_output)\n",
    "            group_outputs.append(group_output)\n",
    "       \n",
    "        # Stack and process through final layers\n",
    "        group_outputs = [x.squeeze(2) for x in group_outputs]\n",
    "        x = torch.stack(group_outputs, dim=2)\n",
    "        x = F.relu(self.conv2d_final(x))\n",
    "        x = self.maxpool_final(x)\n",
    "        x = x.view(x.size(0), -1)  # Flatten for fully connected layers\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = torch.sigmoid(self.fc2(x))\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EEGNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(EEGNet, self).__init__()\n",
    "        # First Convolutional Layer\n",
    "        self.conv1 = nn.Conv2d(1, 6, kernel_size=(5, 5), stride=1)  # Input channels = 1, Output channels = 6\n",
    "        # Mean Pooling Layer\n",
    "        self.pool = nn.AvgPool2d(kernel_size=(2, 2), stride=2)  # Mean pooling with kernel size 2x2\n",
    "        \n",
    "        # Calculate the flattened size for the fully connected layer\n",
    "        # Input size: (batch_size, 1, 19, 5250)\n",
    "        # After conv1: (batch_size, 6, (19-5+1)=15, (5250-5+1)=5246)\n",
    "        # After pool: (batch_size, 6, 15//2=7, 5246//2=2623)\n",
    "        self.fc_input_size = 6 * 7 * 2623\n",
    "        \n",
    "        # Fully Connected Layer\n",
    "        self.fc1 = nn.Linear(self.fc_input_size, 1)  # Assuming binary classification: seizure vs. non-seizure\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.pool(x)\n",
    "        x = torch.flatten(x, 1)  # Flatten all dimensions except batch\n",
    "        x = self.fc1(x)\n",
    "        x = torch.sigmoid(x)  # Sigmoid activation for binary classification\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = EEGNet().to(device)\n",
    "\n",
    "# Loss (mean squared error) and optimizer\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 250\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "    net.train()\n",
    "\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    \n",
    "    for data, labels in train_dataloader:\n",
    "        data, labels = data.to(device), labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = net(data)\n",
    "        loss = criterion(outputs.squeeze(), labels)  \n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        total += labels.size(0)\n",
    "        correct += ((outputs.squeeze() > 0.5) == labels).sum().item()\n",
    "        \n",
    "    loss = running_loss/len(train_dataloader)\n",
    "    accurcay = correct/total\n",
    "\n",
    "    print(f'Epoch {epoch+1}/{num_epochs}, Loss: {loss}, Accuracy: {accurcay}')\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "\n",
    "net.eval()\n",
    "\n",
    "true_labels = []\n",
    "predictions = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data, labels in test_dataloader:\n",
    "        data, labels = data.to(device), labels.to(device)\n",
    "        \n",
    "        # Get model outputs\n",
    "        outputs = net(data)\n",
    "        \n",
    "        # Convert outputs to binary predictions (0 or 1)\n",
    "        preds = (outputs.squeeze() > 0.5).long()\n",
    "        \n",
    "        # Store the predictions and true labels\n",
    "        predictions.extend(preds.cpu().numpy())\n",
    "        true_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "# Calculate standard metrics\n",
    "accuracy = accuracy_score(true_labels, predictions)\n",
    "precision = precision_score(true_labels, predictions)\n",
    "recall = recall_score(true_labels, predictions)\n",
    "f1 = f1_score(true_labels, predictions)\n",
    "roc_auc = roc_auc_score(true_labels, predictions)\n",
    "\n",
    "# Print the results\n",
    "print(f'Test Accuracy: {accuracy}')\n",
    "print(f'Test Precision: {precision}')\n",
    "print(f'Test Recall: {recall}')\n",
    "print(f'Test F1-Score: {f1}')\n",
    "print(f'Test ROC-AUC: {roc_auc}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = ParkNet().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss (mean squared error) and optimizer\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 250\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "    net.train()\n",
    "\n",
    "    total = 0\n",
    "    correct = 0\n",
    "\n",
    "    for data, labels in train_dataloader:\n",
    "        data, labels = data.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = net(data)\n",
    "        loss = criterion(outputs, labels.unsqueeze(1))  \n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        total += labels.size(0)\n",
    "        correct += ((outputs.squeeze() > 0.5) == labels).sum().item()\n",
    "\n",
    "    loss = running_loss/len(train_dataloader)\n",
    "    accurcay = correct/total\n",
    "\n",
    "    print(f'Epoch {epoch+1}/{num_epochs}, Loss: {loss}, Accuracy: {accurcay}')\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WangNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(WangNet, self).__init__()\n",
    "\n",
    "        # input shape: (batch_size, 1, 19, 5250)\n",
    "        self.conv_1 = nn.Conv2d(1, 25, (1, 5))\n",
    "        self.dropout_1 = nn.Dropout(0.25)\n",
    "        self.conv_2 = nn.Conv2d(25, 25, (3, 1))\n",
    "        self.bn1 = nn.BatchNorm2d(25)\n",
    "        self.pool_1 = nn.MaxPool2d((1, 2))\n",
    "        self.conv_3 = nn.Conv2d(25, 50, (1, 5))\n",
    "        self.dropout_2 = nn.Dropout(0.25)\n",
    "        self.conv_4 = nn.Conv2d(50, 50, (3, 1), stride=(2, 1))\n",
    "        self.bn2 = nn.BatchNorm2d(50)\n",
    "        self.pool_2 = nn.MaxPool2d((1, 2))\n",
    "        self.conv_5 = nn.Conv2d(50, 100, (1, 5))\n",
    "        self.dropout_3 = nn.Dropout(0.25)\n",
    "        self.conv_6 = nn.Conv2d(100, 100, (3, 1), stride=(2, 1))\n",
    "        self.bn3 = nn.BatchNorm2d(100)\n",
    "        self.pool_3 = nn.MaxPool2d((1, 2))\n",
    "        self.conv_7 = nn.Conv2d(100, 200, (1, 5))\n",
    "        self.dropout_4 = nn.Dropout(0.25)\n",
    "        self.conv_8 = nn.Conv2d(200, 200, (3, 1), stride=(2, 1))\n",
    "        self.bn4 = nn.BatchNorm2d(200)\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(200 * 648, 256)\n",
    "        self.dropout_5 = nn.Dropout(0.5)\n",
    "        self.fc2 = nn.Linear(256, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv_1(x))\n",
    "        x = self.dropout_1(x)\n",
    "        x = F.relu(self.conv_2(x))\n",
    "        x = self.bn1(x)\n",
    "        x = self.pool_1(x)\n",
    "\n",
    "        x = F.relu(self.conv_3(x))\n",
    "        x = self.dropout_2(x)\n",
    "        x = F.relu(self.conv_4(x))\n",
    "        x = self.bn2(x)\n",
    "        x = self.pool_2(x)\n",
    "\n",
    "        x = F.relu(self.conv_5(x))\n",
    "        x = self.dropout_3(x)\n",
    "        x = F.relu(self.conv_6(x))\n",
    "        x = self.bn3(x)\n",
    "        x = self.pool_3(x)\n",
    "\n",
    "        x = F.relu(self.conv_7(x))\n",
    "        x = self.dropout_4(x)\n",
    "        x = F.relu(self.conv_8(x))\n",
    "        x = self.bn4(x)\n",
    "\n",
    "        x = self.flatten(x)\n",
    "\n",
    "        x = self.fc1(x)\n",
    "        x = self.dropout_5(x)\n",
    "        x = self.fc2(x)\n",
    "\n",
    "        x = torch.sigmoid(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wangNet = WangNet().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Loss (mean squared error) and optimizer\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(wangNet.parameters(), lr=0.0001)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 250\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "    wangNet.train()\n",
    "\n",
    "    total = 0\n",
    "    correct = 0\n",
    "    \n",
    "    for data, labels in train_dataloader:\n",
    "        data, labels = data.to(device), labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = wangNet(data)\n",
    "        loss = criterion(outputs.squeeze(), labels)  \n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        total += labels.size(0)\n",
    "        correct += ((outputs.squeeze() > 0.5) == labels).sum().item()\n",
    "        \n",
    "    loss = running_loss/len(train_dataloader)\n",
    "    accurcay = correct/total\n",
    "\n",
    "    print(f'Epoch {epoch+1}/{num_epochs}, Loss: {loss}, Accuracy: {accurcay}')\n",
    "\n",
    "print('Finished Training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mne",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
